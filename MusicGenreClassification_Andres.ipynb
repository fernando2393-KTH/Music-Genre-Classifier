{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernando2393/DT2119-Final-Project/blob/Andres/MusicGenreClassification_Andres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ARlMjf9rf7o",
        "colab_type": "code",
        "outputId": "6af2c4ad-7f25-40d0-cb99-5aaf72bad6a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVk4S0otr0VY",
        "colab_type": "code",
        "outputId": "a58bb976-4868-479a-c6d4-9285fc625947",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7c754446-19ed-4e2b-995c-5593f99f8257\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7c754446-19ed-4e2b-995c-5593f99f8257\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving constants.py to constants (1).py\n",
            "Saving features.py to features (1).py\n",
            "Saving loader.py to loader (1).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'constants.py': b'# Run locally\\r\\n# DATASETS = \"Datasets/fma_small/\"\\r\\n# METADATA = \"Datasets/fma_metadata/\"\\r\\n# MFCC = \"Datasets/mfcc.csv\"\\r\\n# SPECTROGRAM = \"Datasets/spectrogram.csv\"\\r\\n\\r\\n# # Run on Google Colab\\r\\nDATASETS = \"/content/drive/My Drive/Datasets/fma_small/\"\\r\\nMETADATA = \"/content/drive/My Drive/Datasets/fma_metadata/\"\\r\\nMFCC = \"/content/drive/My Drive/Datasets/mfcc.csv\"\\r\\nSPECTROGRAM = \"/content/drive/My Drive/Datasets/spectrogram.csv\"\\r\\n',\n",
              " 'features.py': b'import pandas as pd\\r\\nimport sklearn\\r\\nimport librosa.display\\r\\nimport librosa\\r\\nimport os\\r\\nfrom tqdm import tqdm\\r\\nimport warnings\\r\\nimport constants as cts\\r\\nimport numpy as np\\r\\n\\r\\n\\r\\ndef compute_feature(mode, filepath):\\r\\n    \"\"\"\\r\\n    This method loads the a music track and computes the mfcc.\\r\\n    :param mode: allows to get either spectrogram or mfcc.\\r\\n    :param filepath: music track.\\r\\n    :return mfcc of the data track.\\r\\n    \"\"\"\\r\\n    if mode == \\'spectrogram\\':\\r\\n        y, sr = librosa.load(filepath, duration=10.5, sr=44100, mono=True, offset=0.5)  # Load 10 seconds\\r\\n        # (same length for every track)\\r\\n        stft = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\\r\\n        mel = librosa.feature.melspectrogram(n_mels=64, sr=sr, S=stft ** 2)\\r\\n        log_mel = librosa.power_to_db(mel, ref=np.max)\\r\\n\\r\\n        return log_mel\\r\\n\\r\\n    y, sr = librosa.load(filepath, duration=28.5, sr=44100, mono=True, offset=0.5)  # Load 28 seconds\\r\\n    # (same length for every track)\\r\\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\\r\\n    mfcc = sklearn.preprocessing.scale(mfcc, axis=1)  # Normalize mfcc to have mean 0 and std 1\\r\\n\\r\\n    return mfcc\\r\\n\\r\\n\\r\\nclass FeatureComputation:\\r\\n\\r\\n    @staticmethod\\r\\n    def preprocessing(mode):\\r\\n        \"\"\"\\r\\n        This method parses the data files, calls compute_mfcc and save them into a .csv file.\\r\\n        \"\"\"\\r\\n        warnings.filterwarnings(\\'ignore\\')\\r\\n        folders = os.listdir(cts.DATASETS)\\r\\n        if \\'.DS_Store\\' in folders:  # MacOS file system check\\r\\n            folders.remove(\\'.DS_Store\\')\\r\\n        folders.sort()\\r\\n        feature_dict = {}\\r\\n        for foldername in tqdm(folders):\\r\\n            files = os.listdir(cts.DATASETS + foldername)\\r\\n            if \\'.DS_Store\\' in files:  # MacOS file system check\\r\\n                files.remove(\\'.DS_Store\\')\\r\\n            files.sort()\\r\\n            for file in files:\\r\\n                if os.path.isdir(cts.DATASETS + foldername):\\r\\n                    key = file.strip(\\'0\\')\\r\\n                    key = key.replace(\\'.mp3\\', \\'\\')\\r\\n                    feature = compute_feature(mode, cts.DATASETS + foldername + \\'/\\' + file)\\r\\n                    feature_dict[int(key)] = feature\\r\\n\\r\\n        df = pd.DataFrame(list(feature_dict.items()), columns=[\\'track\\', mode]).astype(object)\\r\\n        if mode == \\'spectrogram\\':\\r\\n            df.to_pickle(cts.SPECTROGRAM)\\r\\n        else:\\r\\n            df.to_pickle(cts.MFCC)\\r\\n\\r\\n',\n",
              " 'loader.py': b'\"\"\"\\r\\nThis class is in charge of loading the relevant data and splitting the dataset.\\r\\nnote: Check https://nbviewer.jupyter.org/github/mdeff/fma/blob/outputs/usage.ipynb for a deeper understanding of\\r\\n      the data format in the .csv files.\\r\\n\"\"\"\\r\\n\\r\\nimport pandas as pd\\r\\nimport features\\r\\nimport constants as cts\\r\\nfrom pathlib import Path\\r\\n\\r\\n\\r\\nclass Loader:\\r\\n    def __init__(self):\\r\\n        self.features = [\\'mfcc\\', \\'chroma_cens\\', \\'tonnetz\\', \\'spectral_contrast\\',\\r\\n                         [\\'spectral_centroid\\', \\'spectral_bandwidth\\', \\'spectral_rolloff\\'],\\r\\n                         [\\'rmse\\', \\'zcr\\']]  # Main categories of the stored features\\r\\n\\r\\n    @staticmethod\\r\\n    def load_echonest():\\r\\n        \"\"\"\\r\\n        This method loads the data echonest features from the .csv file.\\r\\n        :return echonest features of the data.\\r\\n        \"\"\"\\r\\n        echonest = pd.read_csv(cts.METADATA + \"echonest.csv\", index_col=0, header=[0, 1, 2])\\r\\n\\r\\n        return echonest\\r\\n\\r\\n    @staticmethod\\r\\n    def load_genres():\\r\\n        \"\"\"\\r\\n        This method loads the data genres from the .csv file.\\r\\n        :return genres of the data and top level genres.\\r\\n        \"\"\"\\r\\n        genres = pd.read_csv(cts.METADATA + \"genres.csv\", index_col=0)\\r\\n        top_level = genres[\\'top_level\\'].unique()  # This corresponds to the considered \"top-level genres\"\\r\\n\\r\\n        print(\"There is a total of \" + str(genres.shape[0]) + \" genres.\")\\r\\n        print(\"There is a total of \" + str(len(top_level)) + \" top-level genres.\")\\r\\n\\r\\n        return genres, top_level\\r\\n\\r\\n    @staticmethod\\r\\n    def load_tracks():\\r\\n        \"\"\"\\r\\n        This method loads the data tracks from the .csv file.\\r\\n        :return music tracks.\\r\\n        \"\"\"\\r\\n        tracks = pd.read_csv(cts.METADATA + \"tracks.csv\", index_col=0, header=[0, 1])\\r\\n\\r\\n        return tracks\\r\\n\\r\\n    @staticmethod\\r\\n    def load_features(mode):\\r\\n        if mode == \\'spectrogram\\':\\r\\n            if not Path(cts.SPECTROGRAM).is_file():  # Check if the file exists\\r\\n                compute_ = features.FeatureComputation()\\r\\n                compute_.preprocessing(mode)  # If the file does not exist, create it\\r\\n            mfcc_val = pd.read_pickle(cts.SPECTROGRAM)\\r\\n\\r\\n        else:\\r\\n            if not Path(cts.MFCC).is_file():  # Check if the file exists\\r\\n                compute_ = features.FeatureComputation()\\r\\n                compute_.preprocessing(mode)  # If the file does not exist, create it\\r\\n            mfcc_val = pd.read_pickle(cts.MFCC)\\r\\n\\r\\n        return mfcc_val\\r\\n\\r\\n    @staticmethod\\r\\n    def get_targets(tracks):\\r\\n        \"\"\"\\r\\n        This methods separates the tracks into dataset by means of the \\'cat\\' feature.\\r\\n        :param tracks: the music tracks loaded in the format returned by \\'load_tracks\\'.\\r\\n        :return training set, validation set and test set targets and tracks ID.\\r\\n        \"\"\"\\r\\n        train = tracks[\\'set\\', \\'split\\'] == \\'training\\'  # Training songs\\r\\n        val = tracks[\\'set\\', \\'split\\'] == \\'validation\\'  # Validation songs\\r\\n        test = tracks[\\'set\\', \\'split\\'] == \\'test\\'  # Test songs\\r\\n\\r\\n        y_train = tracks.loc[train, (\\'track\\', \\'genre_top\\')]\\r\\n        y_val = tracks.loc[val, (\\'track\\', \\'genre_top\\')]\\r\\n        y_test = tracks.loc[test, (\\'track\\', \\'genre_top\\')]\\r\\n\\r\\n        return y_train, y_val, y_test\\r\\n\\r\\n\\r\\ndef get_train_val_test(mode=\\'spectrogram\\'):\\r\\n    \"\"\"\\r\\n    :return training, validation and test datasets.\\r\\n    \"\"\"\\r\\n    loader = Loader()\\r\\n    print(\"Calculating \" + mode + \"...\")\\r\\n    mfcc_ = loader.load_features(mode)  # Load the features dataframe of the dataset songs.\\r\\n    tracks = loader.load_tracks()  # Load all the tracks of the big dataset.\\r\\n    y_train, y_val, y_test = loader.get_targets(tracks)  # Load the target values of all the tracks.\\r\\n    # Get training mfcc and labels dataframes.\\r\\n    mfcc_train = mfcc_.loc[mfcc_[\\'track\\'].isin(y_train.index[:].tolist())]\\r\\n    y_train = y_train[mfcc_train[\\'track\\'].to_numpy()]\\r\\n    y_train = y_train[y_train.notna()]\\r\\n    mfcc_train = mfcc_train.loc[mfcc_train[\\'track\\'].isin(y_train.index[:].tolist())]\\r\\n    # Get validation mfcc and labels dataframes.\\r\\n    mfcc_val = mfcc_.loc[mfcc_[\\'track\\'].isin(y_val.index[:].tolist())]\\r\\n    y_val = y_val[mfcc_val[\\'track\\'].to_numpy()]\\r\\n    y_val = y_val[y_val.notna()]\\r\\n    mfcc_val = mfcc_val.loc[mfcc_val[\\'track\\'].isin(y_val.index[:].tolist())]\\r\\n    # Get testing mfcc and labels dataframes.\\r\\n    mfcc_test = mfcc_.loc[mfcc_[\\'track\\'].isin(y_test.index[:].tolist())]\\r\\n    y_test = y_test[mfcc_test[\\'track\\'].to_numpy()]\\r\\n    y_test = y_test[y_test.notna()]\\r\\n    mfcc_test = mfcc_test.loc[mfcc_test[\\'track\\'].isin(y_test.index[:].tolist())]\\r\\n    # Get the mfcc values and convert them to numpy arrays.\\r\\n    x_train = mfcc_train[mode].to_numpy()\\r\\n    x_val = mfcc_val[mode].to_numpy()\\r\\n    x_test = mfcc_test[mode].to_numpy()\\r\\n    # Conver the target values to numpy arrays.\\r\\n    y_train = y_train.to_numpy()\\r\\n    y_val = y_val.to_numpy()\\r\\n    y_test = y_test.to_numpy()\\r\\n\\r\\n    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeW8hEdTsfOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, ZeroPadding2D, ELU, Reshape, GRU\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4sgcgSFs7Yn",
        "colab_type": "code",
        "outputId": "2d0ca791-2f7c-4074-97c0-4cb0e67b5f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import loader\n",
        "(x_train, y_train), (x_val, y_val), (x_test, y_test) = loader.get_train_val_test(mode='spectrogram')\n",
        "print(\"There are the following classes:\")\n",
        "classes = set(y_train.tolist()) & set(y_val.tolist()) & set(y_test.tolist())\n",
        "print(classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating spectrogram...\n",
            "There are the following classes:\n",
            "{'Experimental', 'Electronic', 'Rock', 'International', 'Folk', 'Instrumental', 'Hip-Hop', 'Pop'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycB9CBy43WZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove small samples\n",
        "x_train = np.delete(x_train, [3495, 3496, 3497])\n",
        "y_train = np.delete(y_train, [3495, 3496, 3497])\n",
        "x_train = np.rollaxis(np.dstack(x_train), -1)\n",
        "x_val = np.rollaxis(np.dstack(x_val), -1)\n",
        "x_test = np.rollaxis(np.dstack(x_test), -1)\n",
        "x_train = np.expand_dims(x_train, axis=3)\n",
        "x_val = np.expand_dims(x_val, axis=3)\n",
        "x_test = np.expand_dims(x_test, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDaVTvLQ7gnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encoding of classes\n",
        "dict_labels = {'Electronic': 0, 'Experimental': 1, 'Folk': 2, 'Hip-Hop': 3,\n",
        "                'Instrumental': 4, 'International': 5, 'Pop': 6, 'Rock': 7}\n",
        "y_train = [dict_labels[y_train[i]] for i in range(y_train.shape[0])]\n",
        "y_val = [dict_labels[y_val[i]] for i in range(y_val.shape[0])]\n",
        "y_test = [dict_labels[y_test[i]] for i in range(y_test.shape[0])]\n",
        "y_train = to_categorical(y_train, num_classes=8)\n",
        "y_val = to_categorical(y_val, num_classes=8)\n",
        "y_test = to_categorical(y_test, num_classes=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecU0Y2MM-4xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier:\n",
        "\n",
        "    @staticmethod\n",
        "    def build(inputs, classes):\n",
        "\n",
        "        x = Conv2D(16, (3, 3), strides=(1, 1))(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n",
        "\n",
        "        x = Conv2D(32, (3, 3), strides=(1, 1))(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n",
        "\n",
        "        x = Conv2D(64, (3, 3), strides=(1, 1))(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)\n",
        "        x = Dropout(rate=0.1, trainable=True)(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(64, activation='relu')(x)\n",
        "        x = Dropout(rate=0.3, trainable=True)(x)\n",
        "        outputs = Dense(classes, activation='softmax')(x)\n",
        "\n",
        "        return tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32BE9v7z-753",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1f1e9d9-9f54-49f1-8234-51da94adc7d9"
      },
      "source": [
        "# Training parameters\n",
        "  epochs = 25  # Train for 25 epochs\n",
        "  lr = 0.005  # Initial learning rate\n",
        "  batch_size = 16\n",
        "\n",
        "  inputs = tf.keras.Input(shape=x_train.shape[1:])\n",
        "  model = Classifier.build(inputs, classes=8)\n",
        "  print(\"Summary:\")\n",
        "  print(model.summary())\n",
        "\n",
        "  opt = Adam(lr=lr, decay=lr / epochs)\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "  # Callbacks: early stopping and checkpoint\n",
        "  early_stopping = EarlyStopping(monitor='val_accuracy', verbose=1,\n",
        "                                  patience=10,\n",
        "                                  mode='max',\n",
        "                                  restore_best_weights=True)\n",
        "  filepath = \"weights.{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
        "                                save_best_only=True, mode='max')\n",
        "  callbacks_list = [early_stopping, checkpoint]\n",
        "  history = model.fit(x_train, y_train, batch_size=batch_size,\n",
        "                      validation_data=(x_val, y_val),\n",
        "                      steps_per_epoch=len(x_train) // batch_size,\n",
        "                      callbacks=callbacks_list,\n",
        "                      epochs=epochs, verbose=1)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary:\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 64, 905, 1)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 62, 903, 16)       160       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu (TensorFlow [(None, 62, 903, 16)]     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 61, 902, 16)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 59, 900, 32)       4640      \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_1 (TensorFl [(None, 59, 900, 32)]     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 58, 899, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 56, 897, 64)       18496     \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_2 (TensorFl [(None, 56, 897, 64)]     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 55, 896, 64)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 55, 896, 64)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3153920)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                201850944 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 520       \n",
            "=================================================================\n",
            "Total params: 201,874,760\n",
            "Trainable params: 201,874,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3495 - accuracy: 0.1255\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.12500, saving model to weights.01-0.12.hdf5\n",
            "399/399 [==============================] - 277s 695ms/step - loss: 3.3495 - accuracy: 0.1255 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 2/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3425 - accuracy: 0.1283\n",
            "Epoch 00002: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 253s 633ms/step - loss: 3.3425 - accuracy: 0.1283 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 3/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3551 - accuracy: 0.1250\n",
            "Epoch 00003: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 252s 632ms/step - loss: 3.3551 - accuracy: 0.1250 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 4/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3611 - accuracy: 0.1234\n",
            "Epoch 00004: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 252s 632ms/step - loss: 3.3611 - accuracy: 0.1234 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 5/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3483 - accuracy: 0.1267\n",
            "Epoch 00005: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 253s 634ms/step - loss: 3.3483 - accuracy: 0.1267 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 6/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3563 - accuracy: 0.1246\n",
            "Epoch 00006: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 252s 633ms/step - loss: 3.3563 - accuracy: 0.1246 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 7/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3545 - accuracy: 0.1251\n",
            "Epoch 00007: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 252s 632ms/step - loss: 3.3545 - accuracy: 0.1251 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 8/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3533 - accuracy: 0.1254\n",
            "Epoch 00008: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 252s 632ms/step - loss: 3.3533 - accuracy: 0.1254 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 9/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3533 - accuracy: 0.1254\n",
            "Epoch 00009: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 253s 634ms/step - loss: 3.3533 - accuracy: 0.1254 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 10/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3509 - accuracy: 0.1261\n",
            "Epoch 00010: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 253s 635ms/step - loss: 3.3509 - accuracy: 0.1261 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 11/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 3.3551 - accuracy: 0.1250Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 253s 635ms/step - loss: 3.3551 - accuracy: 0.1250 - val_loss: 3.3550 - val_accuracy: 0.1250\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCP3u9OrNI-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  print(\"\\nHistory dict:\", history.history)\n",
        "\n",
        "  # Evaluate the model on the test data using `evaluate`\n",
        "  print(\"Evaluating model on test data...\")\n",
        "  results = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "  print(\"Test loss:\", results[0])\n",
        "  print(\"Test acc:\", results[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}