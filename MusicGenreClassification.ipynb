{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPKv/e+McHh0B1CG/KA1vKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernando2393/DT2119-Final-Project/blob/master/MusicGenreClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ARlMjf9rf7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "927b16c7-c3d7-44e3-9084-8647d34f87c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVk4S0otr0VY",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "95edf07a-a817-46bd-bcb2-684bf55cd2b0"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cde45f12-f305-45fb-92e9-fe0e71fb1492\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cde45f12-f305-45fb-92e9-fe0e71fb1492\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving constants.py to constants.py\n",
            "Saving loader.py to loader.py\n",
            "Saving main.py to main.py\n",
            "Saving mfcc.py to mfcc.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'constants.py': b'DATASETS = \"/content/drive/My Drive/Datasets/fma_small/\"\\r\\nMETADATA = \"/content/drive/My Drive/Datasets/fma_metadata/\"\\r\\nMFCC = \"/content/drive/My Drive/Datasets/mfcc.csv\"\\r\\n',\n",
              " 'loader.py': b'\"\"\"\\r\\nThis class is in charge of loading the relevant data and splitting the dataset.\\r\\nnote: Check https://nbviewer.jupyter.org/github/mdeff/fma/blob/outputs/usage.ipynb for a deeper understanding of\\r\\n      the data format in the .csv files.\\r\\n\"\"\"\\r\\n\\r\\nimport pandas as pd\\r\\nimport mfcc\\r\\nimport constants as cts\\r\\nfrom pathlib import Path\\r\\n\\r\\n\\r\\nclass Loader:\\r\\n    def __init__(self):\\r\\n        self.features = [\\'mfcc\\', \\'chroma_cens\\', \\'tonnetz\\', \\'spectral_contrast\\',\\r\\n                         [\\'spectral_centroid\\', \\'spectral_bandwidth\\', \\'spectral_rolloff\\'],\\r\\n                         [\\'rmse\\', \\'zcr\\']]  # Main categories of the stored features\\r\\n\\r\\n    @staticmethod\\r\\n    def load_features():\\r\\n        \"\"\"\\r\\n        This method loads the data features from the .csv file.\\r\\n        :return features of the data.\\r\\n        \"\"\"\\r\\n        features = pd.read_csv(cts.METADATA + \"features.csv\", index_col=0, header=[0, 1, 2])\\r\\n\\r\\n        return features\\r\\n\\r\\n    @staticmethod\\r\\n    def load_echonest():\\r\\n        \"\"\"\\r\\n        This method loads the data echonest features from the .csv file.\\r\\n        :return echonest features of the data.\\r\\n        \"\"\"\\r\\n        echonest = pd.read_csv(cts.METADATA + \"echonest.csv\", index_col=0, header=[0, 1, 2])\\r\\n\\r\\n        return echonest\\r\\n\\r\\n    @staticmethod\\r\\n    def load_genres():\\r\\n        \"\"\"\\r\\n        This method loads the data genres from the .csv file.\\r\\n        :return genres of the data and top level genres.\\r\\n        \"\"\"\\r\\n        genres = pd.read_csv(cts.METADATA + \"genres.csv\", index_col=0)\\r\\n        top_level = genres[\\'top_level\\'].unique()  # This corresponds to the considered \"top-level genres\"\\r\\n\\r\\n        print(\"There is a total of \" + str(genres.shape[0]) + \" genres.\")\\r\\n        print(\"There is a total of \" + str(len(top_level)) + \" top-level genres.\")\\r\\n\\r\\n        return genres, top_level\\r\\n\\r\\n    @staticmethod\\r\\n    def load_tracks():\\r\\n        \"\"\"\\r\\n        This method loads the data tracks from the .csv file.\\r\\n        :return music tracks.\\r\\n        \"\"\"\\r\\n        tracks = pd.read_csv(cts.METADATA + \"tracks.csv\", index_col=0, header=[0, 1])\\r\\n\\r\\n        return tracks\\r\\n\\r\\n    @staticmethod\\r\\n    def load_mfcc():\\r\\n        if not Path(cts.MFCC).is_file():  # Check if the file exists\\r\\n            compute_mfcc = mfcc.MfccComputation()\\r\\n            compute_mfcc.preprocessing()  # If the file does not exist, create it\\r\\n        mfcc_val = pd.read_pickle(cts.MFCC)\\r\\n\\r\\n        return mfcc_val\\r\\n\\r\\n    @staticmethod\\r\\n    def get_targets(tracks):\\r\\n        \"\"\"\\r\\n        This methods separates the tracks into dataset by means of the \\'cat\\' feature.\\r\\n        :param tracks: the music tracks loaded in the format returned by \\'load_tracks\\'.\\r\\n        :return training set, validation set and test set targets and tracks ID.\\r\\n        \"\"\"\\r\\n        train = tracks[\\'set\\', \\'split\\'] == \\'training\\'  # Training songs\\r\\n        val = tracks[\\'set\\', \\'split\\'] == \\'validation\\'  # Validation songs\\r\\n        test = tracks[\\'set\\', \\'split\\'] == \\'test\\'  # Test songs\\r\\n\\r\\n        y_train = tracks.loc[train, (\\'track\\', \\'genre_top\\')]\\r\\n        y_val = tracks.loc[val, (\\'track\\', \\'genre_top\\')]\\r\\n        y_test = tracks.loc[test, (\\'track\\', \\'genre_top\\')]\\r\\n\\r\\n        return y_train, y_val, y_test\\r\\n\\r\\n\\r\\ndef get_train_val_test():\\r\\n    \"\"\"\\r\\n    :return training, validation and test datasets.\\r\\n    \"\"\"\\r\\n    loader = Loader()\\r\\n    mfcc_ = loader.load_mfcc()  # Load the mfcc datagrame of the dataset songs.\\r\\n    tracks = loader.load_tracks()  # Load all the tracks of the big dataset.\\r\\n    y_train, y_val, y_test = loader.get_targets(tracks)  # Load the target values of all the tracks.\\r\\n    # Get training mfcc and labels dataframes.\\r\\n    mfcc_train = mfcc_.loc[mfcc_[\\'track\\'].isin(y_train.index[:].tolist())]\\r\\n    y_train = y_train[mfcc_train[\\'track\\'].to_numpy()]\\r\\n    y_train = y_train[y_train.notna()]\\r\\n    mfcc_train = mfcc_train.loc[mfcc_train[\\'track\\'].isin(y_train.index[:].tolist())]\\r\\n    # Get validation mfcc and labels dataframes.\\r\\n    mfcc_val = mfcc_.loc[mfcc_[\\'track\\'].isin(y_val.index[:].tolist())]\\r\\n    y_val = y_val[mfcc_val[\\'track\\'].to_numpy()]\\r\\n    y_val = y_val[y_val.notna()]\\r\\n    mfcc_val = mfcc_val.loc[mfcc_val[\\'track\\'].isin(y_val.index[:].tolist())]\\r\\n    # Get testing mfcc and labels dataframes.\\r\\n    mfcc_test = mfcc_.loc[mfcc_[\\'track\\'].isin(y_test.index[:].tolist())]\\r\\n    y_test = y_test[mfcc_test[\\'track\\'].to_numpy()]\\r\\n    y_test = y_test[y_test.notna()]\\r\\n    mfcc_test = mfcc_test.loc[mfcc_test[\\'track\\'].isin(y_test.index[:].tolist())]\\r\\n    # Get the mfcc values and convert them to numpy arrays.\\r\\n    x_train = mfcc_train[\\'mfcc\\'].to_numpy()\\r\\n    x_val = mfcc_val[\\'mfcc\\'].to_numpy()\\r\\n    x_test = mfcc_test[\\'mfcc\\'].to_numpy()\\r\\n    # Conver the target values to numpy arrays.\\r\\n    y_train = y_train.to_numpy()\\r\\n    y_val = y_val.to_numpy()\\r\\n    y_test = y_test.to_numpy()\\r\\n\\r\\n    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\\r\\n',\n",
              " 'main.py': b'import loader\\r\\n\\r\\n\\r\\ndef main():\\r\\n    (x_train, y_train), (x_val, y_val), (x_test, y_test) = loader.get_train_val_test()\\r\\n    print(\"There are the following classes:\")\\r\\n    classes = set(y_train.tolist()) & set(y_val.tolist()) & set(y_test.tolist())\\r\\n    print(classes)\\r\\n\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    main()\\r\\n',\n",
              " 'mfcc.py': b'import pandas as pd\\r\\nimport sklearn\\r\\nimport librosa.display\\r\\nimport librosa\\r\\nimport os\\r\\nfrom tqdm import tqdm\\r\\nimport warnings\\r\\nimport constants as cts\\r\\n\\r\\n\\r\\ndef compute_mfcc(filepath):\\r\\n    \"\"\"\\r\\n    This method loads the a music track and computes the mfcc.\\r\\n    :param filepath: music track.\\r\\n    :return mfcc of the data track.\\r\\n    \"\"\"\\r\\n    y, sr = librosa.load(filepath, duration=28.5, sr=44100, mono=True, offset=0.5)  # Load 28 seconds\\r\\n    # (same length for every track)\\r\\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\\r\\n    mfcc = sklearn.preprocessing.scale(mfcc, axis=1)  # Normalize mfcc to have mean 0 and std 1\\r\\n\\r\\n    return mfcc\\r\\n\\r\\n\\r\\nclass MfccComputation:\\r\\n\\r\\n    @staticmethod\\r\\n    def preprocessing():\\r\\n        \"\"\"\\r\\n        This method parses the data files, calls compute_mfcc and save them into a .csv file.\\r\\n        \"\"\"\\r\\n        warnings.filterwarnings(\\'ignore\\')\\r\\n        folders = os.listdir(cts.DATASETS)\\r\\n        if \\'.DS_Store\\' in folders:  # MacOS file system check\\r\\n            folders.remove(\\'.DS_Store\\')\\r\\n        folders.sort()\\r\\n        mfcc_dict = {}\\r\\n        for foldername in tqdm(folders):\\r\\n            files = os.listdir(cts.DATASETS + foldername)\\r\\n            if \\'.DS_Store\\' in files:  # MacOS file system check\\r\\n                files.remove(\\'.DS_Store\\')\\r\\n            files.sort()\\r\\n            for file in files:\\r\\n                if os.path.isdir(cts.DATASETS + foldername):\\r\\n                    key = file.strip(\\'0\\')\\r\\n                    key = key.replace(\\'.mp3\\', \\'\\')\\r\\n                    mfcc = compute_mfcc(cts.DATASETS + foldername + \\'/\\' + file)\\r\\n                    mfcc_dict[int(key)] = mfcc\\r\\n\\r\\n        df = pd.DataFrame(list(mfcc_dict.items()), columns=[\\'track\\', \\'mfcc\\']).astype(object)\\r\\n        df.to_pickle(cts.MFCC)\\r\\n\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeW8hEdTsfOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, ZeroPadding2D, ELU, Reshape, GRU\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4sgcgSFs7Yn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c7f1e395-7943-410b-aa63-0d67343e3311"
      },
      "source": [
        "(x_train_prelim, y_train_prelim), (x_val_prelim, y_val_prelim), (x_test_prelim, y_test_prelim) = loader.get_train_val_test()\n",
        "print(\"There are the following classes:\")\n",
        "classes = set(y_train_prelim.tolist()) & set(y_val_prelim.tolist()) & set(y_test_prelim.tolist())\n",
        "print(classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are the following classes:\n",
            "{'Experimental', 'Rock', 'International', 'Folk', 'Electronic', 'Instrumental', 'Pop', 'Hip-Hop'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycB9CBy43WZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove corrupted samples\n",
        "x_train_prelim = np.delete(x_train_prelim, 3495)\n",
        "x_train_prelim = np.delete(x_train_prelim, 3495)\n",
        "x_train_prelim = np.delete(x_train_prelim, 3495)\n",
        "y_train_prelim = np.delete(y_train_prelim, 3495)\n",
        "y_train_prelim = np.delete(y_train_prelim, 3495)\n",
        "y_train_prelim = np.delete(y_train_prelim, 3495)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0mH1Noo3Pgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.zeros([6397 - 3, 13, 2455])\n",
        "x_val = np.zeros([800, 13, 2455])\n",
        "x_test = np.zeros([800, 13, 2455])\n",
        "\n",
        "y_train = np.zeros([6397 - 3])\n",
        "y_val = np.zeros([800])\n",
        "y_test = np.zeros([800])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7lXbpLq8_PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(6397 - 3):\n",
        "  x_train[i] = x_train_prelim[i]\n",
        "\n",
        "for i in range(800):\n",
        "  x_val[i] = x_val_prelim[i]\n",
        "  x_test[i] = x_test_prelim[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE83ZDfEt-B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape((6394, 13, 2455, 1))\n",
        "x_val = x_val.reshape((800, 13, 2455, 1))\n",
        "x_test = x_test.reshape((800, 13, 2455, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDaVTvLQ7gnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_labels = {'Electronic': 0, 'Experimental': 1, 'Folk': 2, 'Hip-Hop': 3,\n",
        "               'Instrumental': 4, 'International': 5, 'Pop': 6, 'Rock': 7}\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  y_train[i] = dict_labels[y_train_prelim[i]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3k2RLnh-m2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y_val_prelim)):\n",
        "  y_val[i] = dict_labels[y_val_prelim[i]]\n",
        "  y_test[i] = dict_labels[y_test_prelim[i]]\n",
        "\n",
        "# One-hot encoding of classes\n",
        "y_train = to_categorical(y_train, num_classes=8)\n",
        "y_val = to_categorical(y_val, num_classes=8)\n",
        "y_test = to_categorical(y_test, num_classes=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVkafgZB98EP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cb014e4-1ee9-4a8f-bb78-926f74b20964"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6394, 13, 2455, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS_Csonw9-MA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72c04ca4-cde8-4a34-eff4-a606842ba158"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecU0Y2MM-4xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier:\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, classes):\n",
        "        \n",
        "        model = Sequential()\n",
        "        inputShape = (height, width, depth)\n",
        "\n",
        "        model.add(Conv2D(16, (3, 3), strides=(1, 1), input_shape=inputShape)) \n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
        "        \n",
        "        model.add(Conv2D(32, (3, 3), strides=(1, 1)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
        "        \n",
        "        model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
        "        model.add(Dropout(0.2))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(classes, activation='softmax'))\n",
        "        \n",
        "        print(\"summary\")\n",
        "        print(model.summary())\n",
        "        \n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32BE9v7z-753",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "b75a3501-02ae-455c-bb97-6bf553181e4f"
      },
      "source": [
        "EPOCHS = 25\n",
        "INIT_LR = 0.005\n",
        "BS = 16\n",
        "\n",
        "model = Classifier.build(width=2455, height=13, depth=1, classes=8)\n",
        "\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR/EPOCHS)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Callbacks: early stopping and checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', verbose=1, \n",
        "                               patience=10,\n",
        "                               mode='max',\n",
        "                               restore_best_weights=True)\n",
        "\n",
        "filepath = \"weights.{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [early_stopping, checkpoint]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "summary\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 11, 2453, 16)      160       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 11, 2453, 16)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 10, 2452, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 2450, 32)       4640      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 2450, 32)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 2449, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 5, 2447, 64)       18496     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 5, 2447, 64)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 2446, 64)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 2446, 64)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 626176)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                40075328  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 520       \n",
            "=================================================================\n",
            "Total params: 40,099,144\n",
            "Trainable params: 40,099,144\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQhUggyt_PGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "b1d78a17-8814-48c5-fb21-744f4449da47"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=BS,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                   steps_per_epoch=len(x_train) // BS,\n",
        "                    callbacks=callbacks_list,\n",
        "                    epochs=EPOCHS, verbose=1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.1205\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.12500, saving model to weights.01-0.12.hdf5\n",
            "399/399 [==============================] - 46s 116ms/step - loss: 0.7085 - accuracy: 0.1205 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 2/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.1235\n",
            "Epoch 00002: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3770 - accuracy: 0.1235 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 3/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1132\n",
            "Epoch 00003: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1132 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 4/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1184\n",
            "Epoch 00004: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1184 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 5/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1223\n",
            "Epoch 00005: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1223 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 6/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1177\n",
            "Epoch 00006: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1177 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 7/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1185\n",
            "Epoch 00007: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1185 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 8/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1156\n",
            "Epoch 00008: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1156 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 9/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1214\n",
            "Epoch 00009: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 113ms/step - loss: 0.3769 - accuracy: 0.1214 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 10/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.1198\n",
            "Epoch 00010: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 112ms/step - loss: 0.3769 - accuracy: 0.1198 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 11/25\n",
            "399/399 [==============================] - ETA: 0s - loss: 0.3768 - accuracy: 0.1225Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.12500\n",
            "399/399 [==============================] - 45s 112ms/step - loss: 0.3768 - accuracy: 0.1225 - val_loss: 0.3768 - val_accuracy: 0.1250\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sshHyXn5Dv7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.pcolormesh(x_test_prelim[300])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}